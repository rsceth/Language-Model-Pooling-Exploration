## Exploring the Impact of Different Pooling Methods on XLM-RoBERTa

## Project structure
```bash
.
â”œâ”€â”€ config                      
â”‚   â”œâ”€â”€ main.yaml                   # Main configuration file
â”‚   â”œâ”€â”€ model                       # Configurations for training model
â”‚   â”‚   â”œâ”€â”€ model1.yaml             # First variation of parameters to train model
â”‚   â”‚   â””â”€â”€ model2.yaml             # Second variation of parameters to train model
â”‚   â””â”€â”€ process                     # Configurations for processing data
â”‚       â”œâ”€â”€ process1.yaml           # First variation of parameters to process data
â”‚       â””â”€â”€ process2.yaml           # Second variation of parameters to process data
â”œâ”€â”€ docs                            # documentation for your project
â”œâ”€â”€ dvc.yaml                        # DVC pipeline
â”œâ”€â”€ .flake8                         # configuration for flake8 - a Python formatter tool
â”œâ”€â”€ .gitignore                      # ignore files that cannot commit to Git
â”œâ”€â”€ Makefile                        # store useful commands to set up the environment
â”œâ”€â”€ pyproject.toml                  # dependencies for poetry
â”œâ”€â”€ README.md                       # describe your project
â”œâ”€â”€ src                             # store source code
â”‚   â”œâ”€â”€ __init__.py                 # make src a Python module 
â”‚   â”œâ”€â”€ data_batcher.py             # process data before training model
â”‚   â””â”€â”€ data_loader.py              # batch the dataset
â”‚   â””â”€â”€ evaluate.py                 # evaluating during training
â”‚   â””â”€â”€ inference.py                # inference script
â”‚   â””â”€â”€ main.py                     # trainer class
â”‚   â””â”€â”€ model.py                    # model architecure
â”‚   â””â”€â”€ pretrainedModel.py          # download/load pretrained model 
â”‚   â””â”€â”€ train_utils.py              # train and evluate model
â”‚   â””â”€â”€ train.py                    # parse paramters to train
â”‚   â””â”€â”€ train.sh                    # parse paramters to train
â”‚   â””â”€â”€ utils.py                    # utils to train
â”‚   â””â”€â”€ visualize.py                # visualize weights for each epoch

```


## Architecure
<img title="LSTM Pooling on XLM-RoBERTa" src="docs/lstm.png">


## ğŸ§© Features

|    |   Feature         | Description |
|----|-------------------|---------------------------------------------------------------|
| âš™ï¸  | **Architecture**  | The project leverages a Python 3.10 environment, utilizing the huggingface package for model training |
| ğŸ”© | **Code Quality**  | The codebase follows best practices with automated testing |
| ğŸ“„ | **Documentation** | [Medium Article](https://medium.com/@eaintthetrsc/tapping-into-xlm-robertas-hidden-potential-14e18a65b8b8)|
| ğŸ§© | **Modularity**    | The codebase is modular with abstract factory modules for data loading, model creating, training and testing, even for inference for single testing|
| ğŸ§ª | **Testing**       | src/inference.py |
| ğŸ“¦ | **Dependencies**  | Key dependencies include Python, HuggingFace and CUML |


## ğŸš€ Getting Started
### ğŸ¤– Usage

<h4>From <code>source</code></h4>

### ğŸ¤– Models

> Model Architecture Detail
> ```console
>  check on src/model.py
> ```

### ğŸš€ Train
> Train
> ```console
> $ bash src/train.sh
> ```

### ğŸ§ª Tests

> Test
> ```console
> $ python src/inference.py
> ```



## ğŸ¤ Contributing

- **[Report Issues](https://github.com/rsceth/Language-Model-Pooling-Exploration/issues)**: Submit bugs found or log feature requests.



## ğŸ“„ License

This project is protected under the [LICENSE](LICENSE) file.